{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK Package - Python - Text Mining Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is NLTK?**\n",
    "\n",
    "NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Python Version: ** Python 2.7\n",
    "\n",
    "** Workbook: ** Jupyter iPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Importing NLTK Package\n",
    "import nltk\n",
    "# If there is an import error, please download the NLTK Package.\n",
    "# nltk.download() <- I have already installed an updated version of NLTK Corpus Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Words:  [u'The', u'Fulton', u'County', u'Grand', u'Jury']\n",
      "The Length of Brown Words:  1161192\n"
     ]
    }
   ],
   "source": [
    "# Testing NLTK\n",
    "from nltk.corpus import brown\n",
    "print \"Sample Words: \", brown.words()[0:5]\n",
    "# Checking the length of brown words\n",
    "print \"The Length of Brown Words: \",len(brown.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__doc__',\n",
       " '__format__',\n",
       " '__getattribute__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__module__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " '__weakref__',\n",
       " '_add',\n",
       " '_c2f',\n",
       " '_delimiter',\n",
       " '_encoding',\n",
       " '_f2c',\n",
       " '_file',\n",
       " '_fileids',\n",
       " '_get_root',\n",
       " '_init',\n",
       " '_map',\n",
       " '_para_block_reader',\n",
       " '_pattern',\n",
       " '_resolve',\n",
       " '_root',\n",
       " '_sent_tokenizer',\n",
       " '_sep',\n",
       " '_tagset',\n",
       " '_unload',\n",
       " '_word_tokenizer',\n",
       " 'abspath',\n",
       " 'abspaths',\n",
       " 'categories',\n",
       " 'citation',\n",
       " 'encoding',\n",
       " 'ensure_loaded',\n",
       " 'fileids',\n",
       " 'license',\n",
       " 'open',\n",
       " 'paras',\n",
       " 'raw',\n",
       " 'readme',\n",
       " 'root',\n",
       " 'sents',\n",
       " 'tagged_paras',\n",
       " 'tagged_sents',\n",
       " 'tagged_words',\n",
       " 'unicode_repr',\n",
       " 'words']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using python's inbuilt function dir(), we find the valid attributes of brown nltk corpus\n",
    "\n",
    "dir(brown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "# NLTK has it's own book's available for testing\n",
    "# Let's check them out.\n",
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_CONTEXT_RE',\n",
       " '_COPY_TOKENS',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__doc__',\n",
       " '__format__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__len__',\n",
       " '__module__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " '__weakref__',\n",
       " '_context',\n",
       " 'collocations',\n",
       " 'common_contexts',\n",
       " 'concordance',\n",
       " 'count',\n",
       " 'dispersion_plot',\n",
       " 'findall',\n",
       " 'generate',\n",
       " 'index',\n",
       " 'name',\n",
       " 'plot',\n",
       " 'readability',\n",
       " 'similar',\n",
       " 'tokens',\n",
       " 'unicode_repr',\n",
       " 'vocab']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check out the valid attributes of one of the text's.\n",
    "dir(text5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length:  45010\n",
      "Words:  [u'now', u'im', u'left', u'with', u'this', u'gay', u'name', u':P', u'PART', u'hey']\n"
     ]
    }
   ],
   "source": [
    "# Let's check out the length of text5 from NLTK package.\n",
    "print \"Length: \", len(text5)\n",
    "\n",
    "print \"Words: \", text5[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sent Tokenize, Word Tokenize, Pos Tagging\n",
    "\n",
    "For any given sentence, how do we tokenize the words to be used as data. Let's take a example sentence from Wikipedia and try to tokenize it using the above mentioned libraries.\n",
    "\n",
    "**Packages Used:** sent_tokenize, word_tokenize, pos_tag\n",
    "\n",
    "**Sample Text:** Text mining, also referred to as text data mining, roughly equivalent to text analytics, is the process of deriving high-quality information from text. High-quality information is typically derived through the devising of patterns and trends through means such as statistical pattern learning. Text mining usually involves the process of structuring the input text (usually parsing, along with the addition of some derived linguistic features and the removal of others, and subsequent insertion into a database), deriving patterns within the structured data, and finally evaluation and interpretation of the output. 'High quality' in text mining usually refers to some combination of relevance, novelty, and interestingness. Typical text mining tasks include text categorization, text clustering, concept/entity extraction, production of granular taxonomies, sentiment analysis, document summarization, and entity relation modeling (i.e., learning relations between named entities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Importing the required packages\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Input sample text\n",
    "sample = \"Text mining, also referred to as text data mining, roughly equivalent to text analytics, is the process of deriving high-quality information from text. High-quality information is typically derived through the devising of patterns and trends through means such as statistical pattern learning. Text mining usually involves the process of structuring the input text (usually parsing, along with the addition of some derived linguistic features and the removal of others, and subsequent insertion into a database), deriving patterns within the structured data, and finally evaluation and interpretation of the output. 'High quality' in text mining usually refers to some combination of relevance, novelty, and interestingness. Typical text mining tasks include text categorization, text clustering, concept/entity extraction, production of granular taxonomies, sentiment analysis, document summarization, and entity relation modeling (i.e., learning relations between named entities).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Output: ['Text mining, also referred to as text data mining, roughly equivalent to text analytics, is the process of deriving high-quality information from text.', 'High-quality information is typically derived through the devising of patterns and trends through means such as statistical pattern learning.', 'Text mining usually involves the process of structuring the input text (usually parsing, along with the addition of some derived linguistic features and the removal of others, and subsequent insertion into a database), deriving patterns within the structured data, and finally evaluation and interpretation of the output.', \"'High quality' in text mining usually refers to some combination of relevance, novelty, and interestingness.\", 'Typical text mining tasks include text categorization, text clustering, concept/entity extraction, production of granular taxonomies, sentiment analysis, document summarization, and entity relation modeling (i.e., learning relations between named entities).']\n",
      "\n",
      "One Sentence: High-quality information is typically derived through the devising of patterns and trends through means such as statistical pattern learning.\n",
      "\n",
      "Lenght:  5\n"
     ]
    }
   ],
   "source": [
    "# Testing Sentence Tokenizer.\n",
    "s1 = sent_tokenize(sample)\n",
    "print \"Full Output:\", s1\n",
    "print \"\\nOne Sentence:\", s1[1]\n",
    "print \"\\nLenght: \", len(s1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As can be noticed from the above output, the sentence tokenizer splits at each and every sentence. That is where ever it find a full stop(.), it stops and breaks that as a part of the data and inputs into an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Output: ['Text', 'mining', ',', 'also', 'referred', 'to', 'as', 'text', 'data', 'mining', ',', 'roughly', 'equivalent', 'to', 'text', 'analytics', ',', 'is', 'the', 'process', 'of', 'deriving', 'high-quality', 'information', 'from', 'text', '.', 'High-quality', 'information', 'is', 'typically', 'derived', 'through', 'the', 'devising', 'of', 'patterns', 'and', 'trends', 'through', 'means', 'such', 'as', 'statistical', 'pattern', 'learning', '.', 'Text', 'mining', 'usually', 'involves', 'the', 'process', 'of', 'structuring', 'the', 'input', 'text', '(', 'usually', 'parsing', ',', 'along', 'with', 'the', 'addition', 'of', 'some', 'derived', 'linguistic', 'features', 'and', 'the', 'removal', 'of', 'others', ',', 'and', 'subsequent', 'insertion', 'into', 'a', 'database', ')', ',', 'deriving', 'patterns', 'within', 'the', 'structured', 'data', ',', 'and', 'finally', 'evaluation', 'and', 'interpretation', 'of', 'the', 'output', '.', \"'High\", 'quality', \"'\", 'in', 'text', 'mining', 'usually', 'refers', 'to', 'some', 'combination', 'of', 'relevance', ',', 'novelty', ',', 'and', 'interestingness', '.', 'Typical', 'text', 'mining', 'tasks', 'include', 'text', 'categorization', ',', 'text', 'clustering', ',', 'concept/entity', 'extraction', ',', 'production', 'of', 'granular', 'taxonomies', ',', 'sentiment', 'analysis', ',', 'document', 'summarization', ',', 'and', 'entity', 'relation', 'modeling', '(', 'i.e.', ',', 'learning', 'relations', 'between', 'named', 'entities', ')', '.']\n",
      "\n",
      "One Word: roughly\n",
      "\n",
      "Lenght:  159\n"
     ]
    }
   ],
   "source": [
    "# Testing Word Toeknizer.\n",
    "s2 = word_tokenize(sample)\n",
    "print \"Full Output:\", s2\n",
    "print \"\\nOne Word:\", s2[11]\n",
    "print \"\\nLenght: \", len(s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As can be noticed from the above output, the word tokenize splits at each and every word. Even commas and full stops are added into different data arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Output: [('Text', 'NNP'), ('mining', 'NN'), (',', ','), ('also', 'RB'), ('referred', 'VBD'), ('to', 'TO'), ('as', 'IN'), ('text', 'NN'), ('data', 'NNS'), ('mining', 'NN'), (',', ','), ('roughly', 'RB'), ('equivalent', 'JJ'), ('to', 'TO'), ('text', 'VB'), ('analytics', 'NNS'), (',', ','), ('is', 'VBZ'), ('the', 'DT'), ('process', 'NN'), ('of', 'IN'), ('deriving', 'VBG'), ('high-quality', 'JJ'), ('information', 'NN'), ('from', 'IN'), ('text', 'NN'), ('.', '.'), ('High-quality', 'NNP'), ('information', 'NN'), ('is', 'VBZ'), ('typically', 'RB'), ('derived', 'VBN'), ('through', 'IN'), ('the', 'DT'), ('devising', 'NN'), ('of', 'IN'), ('patterns', 'NNS'), ('and', 'CC'), ('trends', 'NNS'), ('through', 'IN'), ('means', 'NNS'), ('such', 'JJ'), ('as', 'IN'), ('statistical', 'JJ'), ('pattern', 'NN'), ('learning', 'VBG'), ('.', '.'), ('Text', 'NNP'), ('mining', 'NN'), ('usually', 'RB'), ('involves', 'VBZ'), ('the', 'DT'), ('process', 'NN'), ('of', 'IN'), ('structuring', 'VBG'), ('the', 'DT'), ('input', 'NN'), ('text', 'NN'), ('(', '('), ('usually', 'RB'), ('parsing', 'VBG'), (',', ','), ('along', 'IN'), ('with', 'IN'), ('the', 'DT'), ('addition', 'NN'), ('of', 'IN'), ('some', 'DT'), ('derived', 'VBN'), ('linguistic', 'JJ'), ('features', 'NNS'), ('and', 'CC'), ('the', 'DT'), ('removal', 'NN'), ('of', 'IN'), ('others', 'NNS'), (',', ','), ('and', 'CC'), ('subsequent', 'JJ'), ('insertion', 'NN'), ('into', 'IN'), ('a', 'DT'), ('database', 'NN'), (')', ')'), (',', ','), ('deriving', 'VBG'), ('patterns', 'NNS'), ('within', 'IN'), ('the', 'DT'), ('structured', 'JJ'), ('data', 'NNS'), (',', ','), ('and', 'CC'), ('finally', 'RB'), ('evaluation', 'NN'), ('and', 'CC'), ('interpretation', 'NN'), ('of', 'IN'), ('the', 'DT'), ('output', 'NN'), ('.', '.'), (\"'High\", 'JJ'), ('quality', 'NN'), (\"'\", \"''\"), ('in', 'IN'), ('text', 'JJ'), ('mining', 'NN'), ('usually', 'RB'), ('refers', 'VBZ'), ('to', 'TO'), ('some', 'DT'), ('combination', 'NN'), ('of', 'IN'), ('relevance', 'NN'), (',', ','), ('novelty', 'NN'), (',', ','), ('and', 'CC'), ('interestingness', 'NN'), ('.', '.'), ('Typical', 'JJ'), ('text', 'NN'), ('mining', 'NN'), ('tasks', 'NNS'), ('include', 'VBP'), ('text', 'JJ'), ('categorization', 'NN'), (',', ','), ('text', 'NN'), ('clustering', 'NN'), (',', ','), ('concept/entity', 'NN'), ('extraction', 'NN'), (',', ','), ('production', 'NN'), ('of', 'IN'), ('granular', 'JJ'), ('taxonomies', 'NNS'), (',', ','), ('sentiment', 'NN'), ('analysis', 'NN'), (',', ','), ('document', 'NN'), ('summarization', 'NN'), (',', ','), ('and', 'CC'), ('entity', 'NN'), ('relation', 'NN'), ('modeling', 'NN'), ('(', '('), ('i.e.', 'FW'), (',', ','), ('learning', 'VBG'), ('relations', 'NNS'), ('between', 'IN'), ('named', 'VBN'), ('entities', 'NNS'), (')', ')'), ('.', '.')]\n",
      "\n",
      "One Tag: ('equivalent', 'JJ')\n",
      "\n",
      "Lenght:  159\n"
     ]
    }
   ],
   "source": [
    "# Testing Pos Tagging.\n",
    "# A part-of-speech tagger, or POS-tagger, processes a sequence of words, and attaches a part of speech tag to each word\n",
    "s3 = pos_tag(s2)\n",
    "print \"Full Output:\", s3\n",
    "print \"\\nOne Tag:\", s3[12]\n",
    "print \"\\nLenght: \", len(s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here,\n",
    "\n",
    "**CC <- Coordinating conjunction, NN <- Noun, IN <- Preposition, JJ <- Adjective, VB <- Verbs.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This being just an introduction, I will be working more on Text Mining Algorithms, which can extract data and these can be used to build up a text classifier and work more progressively using Machine Learning Algorithms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
